Title: 2018-05-22
Tags: journal, ai
Date: 2018-05-22 13:18

The LoRa hat for the Raspberry Pi showed up yesterday, so I guess I can started
trialing LoRa for the sensor network. I still need to CAD out a board for the
actual sensor work (something to fit the solar power kit, the Feather, and the
sensors). Speaking of which, I forgot to mention that the Bluefruit battery
experiment concluded, with the battery lasting about four days and four hours.
The 500mAh battery is definitely overkill, but it'll be nice for those days
where maybe there's a lot of rain or it's cloudy several days in a row.

The natural computing stuff has me thinking about how to represent memory. At
first, and I'm just through chapter 1 of the book, I pulled together some ideas
from the AI course. The thinking is that something like a FOL query engine with
the facts stored in a neural network could be used. It would be built around
facts, which are expressed as `Is(Jet1, Plane)` or `At(Jet1, OAK)`. A fact
could be a memory, which is stored in the neural network, or the result of some
action. An action would be expressed similarly (with the addition of pre- and
post-conditions):

```
Fly(Jet1, DEN,
    ;; Pre-conditions.
    [Is(Jet1, Flyable), Is(DEN, Airport),
     Not(At(Jet1, DEN))],
    ;; Post-conditions.
    [Not(At(Jet1, OAK)), At(Jet1, DEN)])
```

I could come with even more pre-conditions, but the general idea is there. One
interesting topic about the post-conditions that I remember from the AI course
is that we have to be sure to make the assertion *not(pre-condition)* in
addition to asserting our mutable post-conditions. That is, flying doesn't
(usually!) alter whether or not *Jet1* is *Flyable* or that *DEN* is an
*Airport*.

Soâ€¦ How to model this with a neural network? *Why* model this as a neural
network? The answer to the second is to develop similar constraints that the
human brain has; there's a fixed storage size that's much lower than what we
could have with a standard query engine. Furthermore, a standard query engine
only knows what it's been told, but it's maybe possible with a deep-learning
network it could make inferences. I don't know, and honestly, I'm humouring the
book's assertions that this is how memory should work in an attempt to broaden
my understanding here. It could definitely be a dead end.

Another thing, once again math comes up as the limiting factor; this time it's
calculus, and it shows up less than a dozen pages into the second chapter. So
that's going to be a problem. Fortunately, it's a surmountable problem.